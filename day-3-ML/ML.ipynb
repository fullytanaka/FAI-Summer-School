{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Arm Bandits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write some code for the following MAB setting: There are `K = 10` arms and the rewards have the following normal distribution: `X1,t ∼ N (1.2, 1), X2,t ∼ N (1.3, 1)` and `X3,t, X3,t, X10,t ∼ N (1.4, 1)`. Say you can play `n = 1000` rounds. Implement some strategies (that are oblivious to the exact distribution). Track their total reward over time and compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mean and standard deviation for each arm\n",
    "K = 10\n",
    "n = 1000\n",
    "means = [1.2, 1.3, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "stds = [1] * K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arm 1: Total Reward = 112.52470931382027, Average Reward = 1.2099431109012933\n",
      "Arm 2: Total Reward = 136.00422937191695, Average Reward = 1.3737800946658278\n",
      "Arm 3: Total Reward = 15.995722230366212, Average Reward = 0.14674974523271755\n",
      "Arm 4: Total Reward = 16.11152618176019, Average Reward = 0.15491852097846337\n",
      "Arm 5: Total Reward = 2.1070623380235736, Average Reward = 0.02194856602107889\n",
      "Arm 6: Total Reward = -11.297658423230544, Average Reward = -0.11185800419040143\n",
      "Arm 7: Total Reward = -3.276041549401194, Average Reward = -0.0394703801132674\n",
      "Arm 8: Total Reward = 3.1244811295641144, Average Reward = 0.031244811295641144\n",
      "Arm 9: Total Reward = -1.5528953453320415, Average Reward = -0.014513040610579828\n",
      "Arm 10: Total Reward = -0.6270497058730335, Average Reward = -0.005806015795120681\n",
      "Overall reward: 269.1140855416145\n"
     ]
    }
   ],
   "source": [
    "# Initialize variables to track total rewards and number of pulls for each arm\n",
    "total_rewards = np.zeros(K)\n",
    "num_pulls = np.zeros(K)\n",
    "\n",
    "# Implement the strategies\n",
    "for t in range(n):\n",
    "    # No strategy implemented, just randomly choose an arm\n",
    "    arm = np.random.randint(K)  # Randomly choose an arm\n",
    "    \n",
    "    # Simulate the reward for the chosen arm\n",
    "    reward = np.random.normal(means[arm], stds[arm])\n",
    "    \n",
    "    # Update the total rewards and number of pulls for the chosen arm\n",
    "    total_rewards[arm] += reward\n",
    "    num_pulls[arm] += 1\n",
    "\n",
    "# Compare the total rewards for each arm\n",
    "overall_reward = 0\n",
    "for arm in range(K):\n",
    "    avg_reward = total_rewards[arm] / num_pulls[arm]\n",
    "    print(f\"Arm {arm+1}: Total Reward = {total_rewards[arm]}, Average Reward = {avg_reward}\")\n",
    "    overall_reward = overall_reward + total_rewards[arm]\n",
    "\n",
    "print(f\"Overall reward: {overall_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arm 1: Total Reward = 42.67214250681836, Average Reward = 1.1853372918560656\n",
      "Arm 2: Total Reward = 1128.7796100408534, Average Reward = 1.2769000113584315\n",
      "Arm 3: Total Reward = 2.8341802066078374, Average Reward = 0.28341802066078375\n",
      "Arm 4: Total Reward = -2.802527420568512, Average Reward = -0.2802527420568512\n",
      "Arm 5: Total Reward = 6.024092827878156, Average Reward = 0.6024092827878156\n",
      "Arm 6: Total Reward = -1.5895073651839708, Average Reward = -0.15895073651839708\n",
      "Arm 7: Total Reward = -6.31486357944099, Average Reward = -0.631486357944099\n",
      "Arm 8: Total Reward = 2.0396909328789157, Average Reward = 0.20396909328789156\n",
      "Arm 9: Total Reward = 3.647167561935246, Average Reward = 0.3647167561935246\n",
      "Arm 10: Total Reward = -4.126686673219396, Average Reward = -0.41266866732193963\n",
      "Overall reward: 1171.1632990385592\n"
     ]
    }
   ],
   "source": [
    "# Initialize variables to track total rewards and number of pulls for each arm\n",
    "total_rewards = np.zeros(K)\n",
    "num_pulls = np.zeros(K)\n",
    "\n",
    "# Implement the strategies\n",
    "for t in range(n):\n",
    "    # Implementing Explore-first strategy\n",
    "    if t < 100:\n",
    "        arm = t % K  # Choose each arm in turn for the first 100 rounds\n",
    "    else:\n",
    "        arm = np.argmax(total_rewards / num_pulls)  # Choose the arm with the highest average reward\n",
    "    \n",
    "    \n",
    "    # Simulate the reward for the chosen arm\n",
    "    reward = np.random.normal(means[arm], stds[arm])\n",
    "    \n",
    "    # Update the total rewards and number of pulls for the chosen arm\n",
    "    total_rewards[arm] += reward\n",
    "    num_pulls[arm] += 1\n",
    "\n",
    "# Compare the total rewards for each arm\n",
    "overall_reward = 0\n",
    "for arm in range(K):\n",
    "    avg_reward = total_rewards[arm] / num_pulls[arm]\n",
    "    print(f\"Arm {arm+1}: Total Reward = {total_rewards[arm]}, Average Reward = {avg_reward}\")\n",
    "    overall_reward = overall_reward + total_rewards[arm]\n",
    "\n",
    "print(f\"Overall reward: {overall_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arm 1: Total Reward = 14.27718925166868, Average Reward = 1.0197992322620486\n",
      "Arm 2: Total Reward = 1114.7725638348797, Average Reward = 1.3114971339233878\n",
      "Arm 3: Total Reward = 0.7882869718547809, Average Reward = 0.04379372065859894\n",
      "Arm 4: Total Reward = 2.714509312229016, Average Reward = 0.1596770183664127\n",
      "Arm 5: Total Reward = -1.200384943414814, Average Reward = -0.1200384943414814\n",
      "Arm 6: Total Reward = 0.14643095599751854, Average Reward = 0.009762063733167903\n",
      "Arm 7: Total Reward = -6.06050131925309, Average Reward = -0.5509546653866445\n",
      "Arm 8: Total Reward = 1.4678299998435325, Average Reward = 0.04586968749511039\n",
      "Arm 9: Total Reward = -4.010093973383736, Average Reward = -0.286435283813124\n",
      "Arm 10: Total Reward = 1.4344642696604801, Average Reward = 0.07549811945581475\n",
      "Overall reward: 1124.3302943600825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_455991/2448472037.py:12: RuntimeWarning: invalid value encountered in divide\n",
      "  arm = np.argmax(total_rewards / num_pulls)  # Choose the arm with the highest average reward\n"
     ]
    }
   ],
   "source": [
    "# Initialize variables to track total rewards and number of pulls for each arm\n",
    "total_rewards = np.zeros(K)\n",
    "num_pulls = np.zeros(K)\n",
    "\n",
    "# Implement the strategies\n",
    "for t in range(n):\n",
    "    # Implementing Episilon-greedy strategy\n",
    "    epsilon = 0.1\n",
    "    if np.random.rand() < epsilon:\n",
    "        arm = np.random.randint(K)  # Randomly choose an arm\n",
    "    else:\n",
    "        arm = np.argmax(total_rewards / num_pulls)  # Choose the arm with the highest average reward\n",
    "    \n",
    "    # Simulate the reward for the chosen arm\n",
    "    reward = np.random.normal(means[arm], stds[arm])\n",
    "    \n",
    "    # Update the total rewards and number of pulls for the chosen arm\n",
    "    total_rewards[arm] += reward\n",
    "    num_pulls[arm] += 1\n",
    "\n",
    "# Compare the total rewards for each arm\n",
    "overall_reward = 0\n",
    "for arm in range(K):\n",
    "    avg_reward = total_rewards[arm] / num_pulls[arm]\n",
    "    print(f\"Arm {arm+1}: Total Reward = {total_rewards[arm]}, Average Reward = {avg_reward}\")\n",
    "    overall_reward = overall_reward + total_rewards[arm]\n",
    "\n",
    "print(f\"Overall reward: {overall_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement UCB and compare. You can use same distributions as before or the one at the end of the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arm 1: Total Reward = 245.71119674307917, Average Reward = 1.266552560531336\n",
      "Arm 2: Total Reward = 1034.8860950023625, Average Reward = 1.3984947229761655\n",
      "Arm 3: Total Reward = -2.2720477423069703, Average Reward = -0.5680119355767426\n",
      "Arm 4: Total Reward = 0.5366357546521549, Average Reward = 0.06707946933151936\n",
      "Arm 5: Total Reward = 1.8019039157364252, Average Reward = 0.18019039157364253\n",
      "Arm 6: Total Reward = 3.5011808971607126, Average Reward = 0.31828917246915567\n",
      "Arm 7: Total Reward = 0.06559290207454117, Average Reward = 0.010932150345756862\n",
      "Arm 8: Total Reward = 3.6682058066379915, Average Reward = 0.3334732551489083\n",
      "Arm 9: Total Reward = 2.086094766872544, Average Reward = 0.23178830743028264\n",
      "Arm 10: Total Reward = -1.3308249225145303, Average Reward = -0.19011784607350432\n",
      "Overall reward: 1288.6540331237547\n"
     ]
    }
   ],
   "source": [
    "# Define the number of arms and rounds\n",
    "K = 10\n",
    "n = 1000\n",
    "\n",
    "# Define the mean and standard deviation for each arm\n",
    "means = [1.2, 1.3, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "stds = [1] * K\n",
    "\n",
    "# Initialize variables to track total rewards and number of pulls for each arm\n",
    "total_rewards = np.zeros(K)\n",
    "num_pulls = np.zeros(K)\n",
    "\n",
    "# Implement the UCB strategy\n",
    "for t in range(n):\n",
    "    # Calculate the upper confidence bound for each arm\n",
    "    ucb_values = [total_rewards[arm] / num_pulls[arm] + np.sqrt(2 * np.log(t + 1) / num_pulls[arm]) if num_pulls[arm] > 0 else float('inf') for arm in range(K)]\n",
    "    \n",
    "    # Choose the arm with the highest upper confidence bound\n",
    "    arm = np.argmax(ucb_values)\n",
    "    \n",
    "    # Simulate the reward for the chosen arm\n",
    "    reward = np.random.normal(means[arm], stds[arm])\n",
    "    \n",
    "    # Update the total rewards and number of pulls for the chosen arm\n",
    "    total_rewards[arm] += reward\n",
    "    num_pulls[arm] += 1\n",
    "\n",
    "# Compare the total rewards for each arm\n",
    "overall_reward = 0\n",
    "for arm in range(K):\n",
    "    avg_reward = total_rewards[arm] / num_pulls[arm]\n",
    "    print(f\"Arm {arm+1}: Total Reward = {total_rewards[arm]}, Average Reward = {avg_reward}\")\n",
    "    overall_reward = overall_reward + total_rewards[arm]\n",
    "\n",
    "print(f\"Overall reward: {overall_reward}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let’s say you want to buy a cat tree from Amazon. One tree has `10` ratings and the average is `5*`. One tree has `10000` ratings and the average is `4.9*`. Which one do you buy? Can you formulate an algorithm that works for all ratings? How would you test how good your algorithm is?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To decide which cat tree to buy based on ratings, you can use a statistical hypothesis testing approach. One commonly used test is the two-sample t-test, which can compare the means of two groups and determine if they are significantly different.\n",
    "\n",
    "Here's an algorithm that works for all ratings:\n",
    "\n",
    "1. Define the two groups:\n",
    "   - Group A: Ratings of the first cat tree (e.g., 10 ratings with an average of 5*)\n",
    "   - Group B: Ratings of the second cat tree (e.g., 10000 ratings with an average of 4.9*)\n",
    "\n",
    "2. Set the significance level (e.g., α = 0.05) to determine the threshold for statistical significance.\n",
    "\n",
    "3. Perform a two-sample t-test to compare the means of the two groups:\n",
    "   - Null hypothesis (H0): The means of the two groups are equal.\n",
    "   - Alternative hypothesis (H1): The means of the two groups are significantly different.\n",
    "\n",
    "4. Calculate the p-value, which represents the probability of observing the data if the null hypothesis is true.\n",
    "\n",
    "5. Compare the p-value to the significance level:\n",
    "   - If the p-value is less than the significance level (p-value < α), reject the null hypothesis and conclude that the means of the two groups are significantly different. Choose the cat tree with the higher average rating.\n",
    "   - If the p-value is greater than or equal to the significance level (p-value ≥ α), fail to reject the null hypothesis and conclude that there is not enough evidence to suggest a significant difference in the means. Choose either cat tree based on other factors.\n",
    "\n",
    "To test how good your algorithm is, you can perform the following steps:\n",
    "1. Collect ratings data for multiple cat trees with known average ratings.\n",
    "2. Apply the algorithm to compare the cat trees and make a decision.\n",
    "3. Compare the decision made by the algorithm with the known ground truth (e.g., the cat tree with the highest average rating).\n",
    "4. Calculate the accuracy or success rate of the algorithm by dividing the number of correct decisions by the total number of test cases.\n",
    "5. Repeat the testing process with different sets of cat trees to evaluate the algorithm's performance across various scenarios.\n",
    "\n",
    "By testing the algorithm on different sets of cat trees with known ratings, you can assess its effectiveness in making accurate decisions based on the provided ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How would we implement a Tic-Tac-Toe AI?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can implement an AI for Tic-Tac-Toe using an algorithm very similar to Multi-Arm Bandits (MABs), called 'Monte Carlo Tree Search' (MCTS). MCTS is a tree-traversal algorithm whose objective is to find a winning state (i.e. maximise rewards). It follows the exploration/exploitation principles used in MABs.\n",
    "\n",
    "The use of Monte Carlo tree search (MCTS) starts from competitive environments where two or more agents have conflicting goals. MCTS has four stages that repeat a given number of times: selection, expansion, simulation and backpropagation.\n",
    "\n",
    "Consider a tree graph. The MCTS algorithm begins by selecting a node. The node selected depends on a policy defined as an argument for the algorithm, as some algorithms will prefer to either \"exploit\" a potentially good node, or \"explore\" a new node. After selection, expansion occurs where the possible actions within that selected node are given as children nodes. Simulation then plays out one of the child nodes by making random moves. If the computation budget permits it, the simulation will go on until there is a decisive game result. If not, a depth limit can be given as an argument and rather than using the game result, rewards are used to give a value to a position and tell the A.I. how favourable it is to reach this position. The decisive game result, or resulting state's value, is then \"backpropagated\" to the tree, updating the selected node's value and visit count.\n",
    "\n",
    "In the context of the starting state of Tic-Tac-Toe, the MCTS algorithm will take input a game state, 'select' one of its possible moves (in the opening state, there are 9 possible moves), 'expand' its opponent's possible moves (after the first move is made, 8 possible moves are made), and then 'simulate' the rest of the game through random play. The total possible states of Tic-Tac-Toe is 3^9, which while large for human calculation, is fine for a computer to roll out to its entirety (i.e. a state where the game has ended). After it finishes simulation, it backpropagates the result of the game (win, loss, draw) and then 'backpropagates' this result and assigns it to the initial starting move. This repeats for a given number of times.\n",
    "\n",
    "Hopefully, you can see the similarity between MCTS and MABs, where rather than using a normal distribution, we take into account the distribution of possible wins, draws, and losses from the given move.\n",
    "\n",
    "In the context of Chess, where the state space is many orders larger, this changes..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So, what about Chess?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to be more creative, instead of taking wins, losses, and draws. This is where reward modelling comes in. A naive (but still relatively useful) approach is to consider the values of pieces on the board and use them as rewards. If you were ever taught chess, you might consider the following piece values:\n",
    "\n",
    "Pawn: 1\n",
    "Knight: 3\n",
    "Bishop: 3*\n",
    "Rook: 5\n",
    "Queen: 9\n",
    "\n",
    "<i>*More modern chess books may say that the Bishop is slightly higher. Indeed, world chess champions Bobby Fischer and Garry Kasparov would say the Bishop is worth 3.25 and 3.15 respectively. </i>\n",
    "\n",
    "...and instead of rolling out to a win, you might specify how long to roll out for. In other words, depth. A common depth good enough for most levels of Chess is 12 moves ahead (although, the highest levels of play may favour much higher values, like 25 moves ahead). This is entirely dependent on computation power available to you.\n",
    "\n",
    "After reaching your given expansion depth, you then evaluate your position. Who has more total piece value? If the AI has more total piece value, then this game state is seen as favourable, and backpropagates this state value.\n",
    "\n",
    "Actual Chess AI take into account much more than my proposed naive approach here. For example, they may consider how many squares each piece controls, rather than absolute piece value (Consider a Knight. A Knight on the centre of the board controls 8 squares, but if you place the knight on the corner of the board, it only controls 4 squares). They may also include separate rewards for moves lead to forced checkmate (i.e. reduce the expansion space to very few to one, and all expansions lead to checkmate). How many pawns you have in the centre, how close a pawn is to the edge of the board (resulting in a Queen in the next few moves). The possibilities of reward modelling is endless, and is up to the reward modeller to determine what to use as rewards.\n",
    "\n",
    "#### Aside: Deep Learning\n",
    "Chess AI that utilise deep learning (AlphaZero) model their rewards using deep learning! They learn these rewards by playing against itself many, many times (AlphaZero famously ran on a super computer for only four hours to become the AI chess champion, only training against itself)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
